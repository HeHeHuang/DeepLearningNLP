{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_NN_regressiong_with_Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOtryPheQHVH0HD5qdl5bOA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HeHeHuang/DeepLearningNLP/blob/main/01_NN_regressiong_with_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bas6iRlJeOC_",
        "outputId": "24b449fe-57fd-47a1-a57a-29d89b4419fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## createing data to view and fit"
      ],
      "metadata": {
        "id": "WhB5WU1IgjR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = np.array([-7.0,-4.0,-1.0,2.0,5.0,8.0,11.0,14.0])\n",
        "\n",
        "y = np.array([3.0,6.0,9.0,12.0,15.0,18.0,21.0,24.0])\n",
        "\n",
        "plt.scatter(X,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "V25JoJtNejyK",
        "outputId": "fbd4c3d7-b09a-4b2c-c2ab-2562648ab554"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fa52a67d650>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y == X+ 10 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYGULeOJhfwm",
        "outputId": "0216091b-8fc2-435a-e1af-ed927643902a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True,  True,  True,  True,  True,  True])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input & Output Shape"
      ],
      "metadata": {
        "id": "SDO24Xa5jKrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "X = tf.constant(tf.cast(X,dtype=tf.float32))\n",
        "y = tf.constant(tf.cast(y,dtype=tf.float32))\n",
        "X,y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rmy2bqVAjOjM",
        "outputId": "8b7073ea-74b0-46e2-b760-3af091ecefa0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(8,), dtype=float32, numpy=array([-7., -4., -1.,  2.,  5.,  8., 11., 14.], dtype=float32)>,\n",
              " <tf.Tensor: shape=(8,), dtype=float32, numpy=array([ 3.,  6.,  9., 12., 15., 18., 21., 24.], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0],y[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGkFCL06jOyE",
        "outputId": "ca2b90fc-3a66-4547-e275-82d8e2762e0f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(), dtype=float32, numpy=-7.0>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=3.0>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = X[0].shape\n",
        "input_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqnDNMyBnO2C",
        "outputId": "4860722c-49de-4349-f70f-8a533b847111"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X,y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPY8CL2iuUqe",
        "outputId": "0f90b28d-6357-40f3-e45d-c71c0700cd3a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(8,), dtype=float64, numpy=array([-7., -4., -1.,  2.,  5.,  8., 11., 14.])>,\n",
              " <tf.Tensor: shape=(8,), dtype=float64, numpy=array([ 3.,  6.,  9., 12., 15., 18., 21., 24.])>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "as you can see here, the x is scalar , y is scalar\n",
        "> there is no shape, how we build the input shape"
      ],
      "metadata": {
        "id": "svwspdcSjO6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## steps in modelling with Tensorflow\n",
        "\n",
        "* create model: sequential: input layer, output layer, hidden layer \n",
        "* compile model: loss function, optimizer, evaluation matric\n",
        "* fit model: epochs \n",
        "\n"
      ],
      "metadata": {
        "id": "pP2_wiCNjO9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> two way to create Sequential() "
      ],
      "metadata": {
        "id": "8XdJRAvrqs9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "# create model \n",
        "model = tf.keras.Sequential(\n",
        "    tf.keras.layers.Dense(1)\n",
        ")\n",
        "\n",
        "\n",
        "# compile model:\n",
        "model.compile(\n",
        "    loss = tf.keras.losses.mae,\n",
        "    optimizer =tf.keras.optimizers.SGD(),\n",
        "    metrics=['mae'])\n",
        "\n",
        "\n",
        "# fit data to model\n",
        "#model.fit(X,y, epochs = 5 )    #\"dense_9\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n",
        "    \n",
        "\n",
        "model.fit(tf.expand_dims(X, axis=-1), y, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTYsVzSSnLUF",
        "outputId": "97aa36ec-e85f-47ed-e067-b080e8bf767c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 11.2219 - mae: 11.2219\n",
            "Epoch 2/5\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 11.0894 - mae: 11.0894\n",
            "Epoch 3/5\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 10.9569 - mae: 10.9569\n",
            "Epoch 4/5\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 10.8244 - mae: 10.8244\n",
            "Epoch 5/5\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 10.6919 - mae: 10.6919\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa521fde8d0>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X,y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7KVrpTl2V_T",
        "outputId": "b324fb75-6858-405b-ea73-008ec2f0eb1b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(8,), dtype=float32, numpy=array([-7., -4., -1.,  2.,  5.,  8., 11., 14.], dtype=float32)>,\n",
              " <tf.Tensor: shape=(8,), dtype=float32, numpy=array([ 3.,  6.,  9., 12., 15., 18., 21., 24.], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict([17.0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KyqDc412Ry6",
        "outputId": "5500ab7d-5a70-47f6-a24f-9346cca7304d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[14.089865]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Problem in input shape "
      ],
      "metadata": {
        "id": "FkcfGUoj24VV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:**  \n",
        "there is problem that: pass X is less dim for the layers. \n",
        "> Use the expand_dims to add dimension, axis means which dimension you want to add "
      ],
      "metadata": {
        "id": "xJxiYTeaxVXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_more_dims = tf.expand_dims(X,axis=-1)\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxCcUPnxw71n",
        "outputId": "525f4a1f-57fa-4b50-fd80-65f166b2130a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(8,), dtype=float32, numpy=array([-7., -4., -1.,  2.,  5.,  8., 11., 14.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_more_dims"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H52wozT0jPBN",
        "outputId": "aa1268a7-9105-409f-cbfe-b1d04712c90e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(8, 1), dtype=float32, numpy=\n",
              "array([[-7.],\n",
              "       [-4.],\n",
              "       [-1.],\n",
              "       [ 2.],\n",
              "       [ 5.],\n",
              "       [ 8.],\n",
              "       [11.],\n",
              "       [14.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improve model\n",
        "> if we use the model we build and predict. the value is wrong. seems the NN didnt find the current pattern \n",
        "\n",
        "\n",
        "\n",
        "* **Create model:** more complex model: more hidden layer or more neurons, different activation function\n",
        "* **Compile model:** use different optimizer, lower learning rate of optimization function \n",
        "* **Fit data to model:**  increase epochs (more time to learning) or more data to learning"
      ],
      "metadata": {
        "id": "-XdHKrPnjPD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Increase epochs"
      ],
      "metadata": {
        "id": "D1slwtTV8eL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(tf.expand_dims(X, axis=-1), y, epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOIZGLKD3RoY",
        "outputId": "823cea02-b571-4e96-c4e8-054194d5b273"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 10.5594 - mae: 10.5594\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 10.4269 - mae: 10.4269\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 10.2944 - mae: 10.2944\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 10.1619 - mae: 10.1619\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 10.0294 - mae: 10.0294\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.8969 - mae: 9.8969\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 9.7644 - mae: 9.7644\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.6319 - mae: 9.6319\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.4994 - mae: 9.4994\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.3669 - mae: 9.3669\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.2344 - mae: 9.2344\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 9.1019 - mae: 9.1019\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.9694 - mae: 8.9694\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.8369 - mae: 8.8369\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 8.7044 - mae: 8.7044\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.5719 - mae: 8.5719\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.4394 - mae: 8.4394\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.3069 - mae: 8.3069\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.1744 - mae: 8.1744\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.0419 - mae: 8.0419\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.9094 - mae: 7.9094\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.7769 - mae: 7.7769\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.6444 - mae: 7.6444\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.5119 - mae: 7.5119\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.3794 - mae: 7.3794\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.2750 - mae: 7.2750\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 7.2694 - mae: 7.2694\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.2638 - mae: 7.2638\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.2581 - mae: 7.2581\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.2525 - mae: 7.2525\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.2469 - mae: 7.2469\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.2412 - mae: 7.2412\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.2356 - mae: 7.2356\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.2300 - mae: 7.2300\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 7.2244 - mae: 7.2244\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.2188 - mae: 7.2188\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.2131 - mae: 7.2131\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.2075 - mae: 7.2075\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.2019 - mae: 7.2019\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.1962 - mae: 7.1962\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.1906 - mae: 7.1906\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.1850 - mae: 7.1850\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.1794 - mae: 7.1794\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.1737 - mae: 7.1737\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.1681 - mae: 7.1681\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.1625 - mae: 7.1625\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.1569 - mae: 7.1569\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.1512 - mae: 7.1512\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.1456 - mae: 7.1456\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.1400 - mae: 7.1400\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.1344 - mae: 7.1344\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.1288 - mae: 7.1288\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.1231 - mae: 7.1231\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.1175 - mae: 7.1175\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.1119 - mae: 7.1119\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.1062 - mae: 7.1062\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.1006 - mae: 7.1006\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 7.0950 - mae: 7.0950\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.0894 - mae: 7.0894\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.0838 - mae: 7.0838\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.0781 - mae: 7.0781\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.0725 - mae: 7.0725\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.0669 - mae: 7.0669\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.0612 - mae: 7.0612\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.0556 - mae: 7.0556\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.0500 - mae: 7.0500\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.0444 - mae: 7.0444\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.0388 - mae: 7.0388\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.0331 - mae: 7.0331\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.0275 - mae: 7.0275\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.0219 - mae: 7.0219\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.0163 - mae: 7.0163\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.0106 - mae: 7.0106\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.0050 - mae: 7.0050\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.9994 - mae: 6.9994\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.9938 - mae: 6.9938\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.9881 - mae: 6.9881\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.9825 - mae: 6.9825\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.9769 - mae: 6.9769\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.9712 - mae: 6.9712\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.9656 - mae: 6.9656\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.9600 - mae: 6.9600\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.9544 - mae: 6.9544\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.9488 - mae: 6.9488\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.9431 - mae: 6.9431\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.9375 - mae: 6.9375\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.9319 - mae: 6.9319\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.9262 - mae: 6.9262\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.9206 - mae: 6.9206\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.9150 - mae: 6.9150\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.9094 - mae: 6.9094\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.9038 - mae: 6.9038\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8981 - mae: 6.8981\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8925 - mae: 6.8925\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8869 - mae: 6.8869\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.8812 - mae: 6.8812\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.8756 - mae: 6.8756\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8700 - mae: 6.8700\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8644 - mae: 6.8644\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.8588 - mae: 6.8588\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.8531 - mae: 6.8531\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8475 - mae: 6.8475\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.8419 - mae: 6.8419\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8363 - mae: 6.8363\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.8306 - mae: 6.8306\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.8250 - mae: 6.8250\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.8194 - mae: 6.8194\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.8138 - mae: 6.8138\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.8081 - mae: 6.8081\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8025 - mae: 6.8025\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7969 - mae: 6.7969\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7913 - mae: 6.7913\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.7856 - mae: 6.7856\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7800 - mae: 6.7800\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7744 - mae: 6.7744\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7688 - mae: 6.7688\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7631 - mae: 6.7631\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.7575 - mae: 6.7575\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7519 - mae: 6.7519\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7463 - mae: 6.7463\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7406 - mae: 6.7406\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7350 - mae: 6.7350\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.7294 - mae: 6.7294\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7238 - mae: 6.7238\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7181 - mae: 6.7181\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.7125 - mae: 6.7125\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.7069 - mae: 6.7069\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.7013 - mae: 6.7013\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.6956 - mae: 6.6956\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.6900 - mae: 6.6900\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.6844 - mae: 6.6844\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.6788 - mae: 6.6788\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.6731 - mae: 6.6731\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.6675 - mae: 6.6675\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.6619 - mae: 6.6619\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.6562 - mae: 6.6562\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.6506 - mae: 6.6506\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 6.6450 - mae: 6.6450\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.6394 - mae: 6.6394\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.6337 - mae: 6.6337\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.6281 - mae: 6.6281\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.6225 - mae: 6.6225\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.6169 - mae: 6.6169\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.6112 - mae: 6.6112\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.6056 - mae: 6.6056\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.6000 - mae: 6.6000\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.5944 - mae: 6.5944\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.5887 - mae: 6.5887\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.5831 - mae: 6.5831\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.5775 - mae: 6.5775\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 6.5719 - mae: 6.5719\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 6.5662 - mae: 6.5662\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.5606 - mae: 6.5606\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.5550 - mae: 6.5550\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.5494 - mae: 6.5494\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.5437 - mae: 6.5437\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.5381 - mae: 6.5381\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.5325 - mae: 6.5325\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.5269 - mae: 6.5269\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.5212 - mae: 6.5212\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.5156 - mae: 6.5156\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.5100 - mae: 6.5100\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.5044 - mae: 6.5044\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.4987 - mae: 6.4987\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.4931 - mae: 6.4931\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.4875 - mae: 6.4875\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.4819 - mae: 6.4819\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.4762 - mae: 6.4762\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.4706 - mae: 6.4706\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.4650 - mae: 6.4650\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.4594 - mae: 6.4594\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.4537 - mae: 6.4537\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.4481 - mae: 6.4481\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.4425 - mae: 6.4425\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.4369 - mae: 6.4369\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.4312 - mae: 6.4312\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.4256 - mae: 6.4256\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.4200 - mae: 6.4200\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.4144 - mae: 6.4144\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.4087 - mae: 6.4087\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 6.4031 - mae: 6.4031\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.3975 - mae: 6.3975\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.3919 - mae: 6.3919\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.3862 - mae: 6.3862\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.3806 - mae: 6.3806\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.3750 - mae: 6.3750\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.3694 - mae: 6.3694\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.3637 - mae: 6.3637\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.3581 - mae: 6.3581\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.3525 - mae: 6.3525\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.3469 - mae: 6.3469\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.3412 - mae: 6.3412\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.3356 - mae: 6.3356\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.3300 - mae: 6.3300\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.3244 - mae: 6.3244\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.3187 - mae: 6.3187\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.3131 - mae: 6.3131\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.3075 - mae: 6.3075\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 6.3019 - mae: 6.3019\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.2962 - mae: 6.2962\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa521e89450>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X,y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6LJOGY78oht",
        "outputId": "8f204ff3-2eae-476e-809f-2a719a737989"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(8,), dtype=float32, numpy=array([-7., -4., -1.,  2.,  5.,  8., 11., 14.], dtype=float32)>,\n",
              " <tf.Tensor: shape=(8,), dtype=float32, numpy=array([ 3.,  6.,  9., 12., 15., 18., 21., 24.], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict([17])  #  more close to 27 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K87AkMPyjPG4",
        "outputId": "492097fc-f6e3-4611-b391-93d03b379c86"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[30.527359]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### improvement in creating model "
      ],
      "metadata": {
        "id": "kiPFm8D2BA1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create model \n",
        "model_1 = tf.keras.Sequential([    # the layers between input layer and output layer will hidden layer\n",
        "    tf.keras.layers.Dense(10),     # add hidden layers wit 10 neurous                       \n",
        "    tf.keras.layers.Dense(1,activation='relu') # add activation function                            \n",
        "])\n",
        "#model compile\n",
        "model_1.compile(\n",
        "    loss = tf.keras.losses.mae,\n",
        "    optimizer = tf.keras.optimizers.SGD(),\n",
        "    metrics = ['mae']\n",
        ")\n",
        "#model fitting \n",
        "model_1.fit(tf.expand_dims(X,axis=-1),y,epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL0KGh6xjPLB",
        "outputId": "55e5fab4-4330-43c2-ad18-b7c3c8315a93"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 0s 340ms/step - loss: 12.1896 - mae: 12.1896\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 11.1966 - mae: 11.1966\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 10.1854 - mae: 10.1854\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 9.1456 - mae: 9.1456\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 8.0671 - mae: 8.0671\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.9390 - mae: 6.9390\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 5.7502 - mae: 5.7502\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.7676 - mae: 4.7676\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.6435 - mae: 4.6435\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.6545 - mae: 4.6545\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.6166 - mae: 4.6166\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.6686 - mae: 4.6686\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.5897 - mae: 4.5897\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.6829 - mae: 4.6829\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.5973 - mae: 4.5973\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 4.6614 - mae: 4.6614\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.6114 - mae: 4.6114\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.6345 - mae: 4.6345\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.6256 - mae: 4.6256\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 4.6074 - mae: 4.6074\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.6399 - mae: 4.6399\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.5802 - mae: 4.5802\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.6543 - mae: 4.6543\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.5692 - mae: 4.5692\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.6514 - mae: 4.6514\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.5835 - mae: 4.5835\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.6242 - mae: 4.6242\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.5979 - mae: 4.5979\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.5969 - mae: 4.5969\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.6123 - mae: 4.6123\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.5694 - mae: 4.5694\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 4.6269 - mae: 4.6269\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.5422 - mae: 4.5422\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.6401 - mae: 4.6401\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.5567 - mae: 4.5567\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.6127 - mae: 4.6127\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.5713 - mae: 4.5713\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.5851 - mae: 4.5851\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.5859 - mae: 4.5859\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.5573 - mae: 4.5573\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 4.6007 - mae: 4.6007\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.5293 - mae: 4.5293\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.6156 - mae: 4.6156\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.5310 - mae: 4.5310\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 4.5998 - mae: 4.5998\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5457 - mae: 4.5457\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5719 - mae: 4.5719\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5606 - mae: 4.5606\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5438 - mae: 4.5438\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5755 - mae: 4.5755\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5155 - mae: 4.5155\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5906 - mae: 4.5906\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5063 - mae: 4.5063\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.5856 - mae: 4.5856\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.5212 - mae: 4.5212\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5574 - mae: 4.5574\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.5363 - mae: 4.5363\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.5289 - mae: 4.5289\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5514 - mae: 4.5514\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.5003 - mae: 4.5003\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.5667 - mae: 4.5667\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4826 - mae: 4.4826\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5701 - mae: 4.5701\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4977 - mae: 4.4977\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5415 - mae: 4.5415\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5130 - mae: 4.5130\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5127 - mae: 4.5127\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5283 - mae: 4.5283\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4838 - mae: 4.4838\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5438 - mae: 4.5438\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4599 - mae: 4.4599\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5531 - mae: 4.5531\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4753 - mae: 4.4753\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.5242 - mae: 4.5242\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4907 - mae: 4.4907\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4951 - mae: 4.4951\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5063 - mae: 4.5063\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4658 - mae: 4.4658\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5219 - mae: 4.5219\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4382 - mae: 4.4382\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.5349 - mae: 4.5349\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4537 - mae: 4.4537\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.5056 - mae: 4.5056\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4694 - mae: 4.4694\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.4761 - mae: 4.4761\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4851 - mae: 4.4851\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4464 - mae: 4.4464\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.5010 - mae: 4.5010\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.4174 - mae: 4.4174\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.5152 - mae: 4.5152\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.4331 - mae: 4.4331\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4855 - mae: 4.4855\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.4490 - mae: 4.4490\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4556 - mae: 4.4556\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.4649 - mae: 4.4649\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4256 - mae: 4.4256\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4810 - mae: 4.4810\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3975 - mae: 4.3975\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4941 - mae: 4.4941\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4134 - mae: 4.4134\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4640 - mae: 4.4640\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4294 - mae: 4.4294\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.4337 - mae: 4.4337\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4456 - mae: 4.4456\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4033 - mae: 4.4033\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4619 - mae: 4.4619\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3784 - mae: 4.3784\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4715 - mae: 4.4715\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3945 - mae: 4.3945\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4411 - mae: 4.4411\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4108 - mae: 4.4108\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.4104 - mae: 4.4104\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.4271 - mae: 4.4271\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.3795 - mae: 4.3795\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.4436 - mae: 4.4436\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.3601 - mae: 4.3601\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.4475 - mae: 4.4475\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.3764 - mae: 4.3764\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4166 - mae: 4.4166\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.3929 - mae: 4.3929\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3855 - mae: 4.3855\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4095 - mae: 4.4095\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3542 - mae: 4.3542\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4262 - mae: 4.4262\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.3426 - mae: 4.3426\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4221 - mae: 4.4221\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3591 - mae: 4.3591\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.3907 - mae: 4.3907\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3758 - mae: 4.3758\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.3591 - mae: 4.3591\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3926 - mae: 4.3926\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.3273 - mae: 4.3273\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4096 - mae: 4.4096\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.3258 - mae: 4.3258\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3951 - mae: 4.3951\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3426 - mae: 4.3426\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.3633 - mae: 4.3633\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3595 - mae: 4.3595\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3312 - mae: 4.3312\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3765 - mae: 4.3765\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.2989 - mae: 4.2989\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.3937 - mae: 4.3937\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.3098 - mae: 4.3098\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.3666 - mae: 4.3666\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.3268 - mae: 4.3268\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.3343 - mae: 4.3343\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.3439 - mae: 4.3439\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.3018 - mae: 4.3018\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3611 - mae: 4.3611\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.2774 - mae: 4.2774\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.3691 - mae: 4.3691\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.2945 - mae: 4.2945\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 4.3365 - mae: 4.3365\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.3117 - mae: 4.3117\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 4.3037 - mae: 4.3037\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.3291 - mae: 4.3291\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.2706 - mae: 4.2706\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.3465 - mae: 4.3465\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 4.2626 - mae: 4.2626\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.3380 - mae: 4.3380\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.2799 - mae: 4.2799\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3049 - mae: 4.3049\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.2973 - mae: 4.2973\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.2715 - mae: 4.2715\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3149 - mae: 4.3149\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.2379 - mae: 4.2379\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3326 - mae: 4.3326\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.2483 - mae: 4.2483\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.3047 - mae: 4.3047\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.2579 - mae: 4.2579\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.2754 - mae: 4.2754\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.2676 - mae: 4.2676\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.2459 - mae: 4.2459\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.2775 - mae: 4.2775\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.2161 - mae: 4.2161\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.2850 - mae: 4.2850\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.1916 - mae: 4.1916\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.2830 - mae: 4.2830\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.1810 - mae: 4.1810\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.2661 - mae: 4.2661\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 4.1791 - mae: 4.1791\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.2413 - mae: 4.2413\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.1771 - mae: 4.1771\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.2162 - mae: 4.2162\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.1753 - mae: 4.1753\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.1909 - mae: 4.1909\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 4.1735 - mae: 4.1735\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.1653 - mae: 4.1653\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.1717 - mae: 4.1717\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.1394 - mae: 4.1394\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 4.1700 - mae: 4.1700\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.1132 - mae: 4.1132\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.1684 - mae: 4.1684\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0867 - mae: 4.0867\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.1668 - mae: 4.1668\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0637 - mae: 4.0637\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.1605 - mae: 4.1605\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.0621 - mae: 4.0621\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.1337 - mae: 4.1337\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0605 - mae: 4.0605\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa521e7ac10>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.predict([17])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELWq-0g6jPOq",
        "outputId": "3f384e48-124b-423b-995e-07e7308b2db1"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa525f75200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[30.312168]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### imporvement in model compile & model fitting\n"
      ],
      "metadata": {
        "id": "EhCP-1cujPRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create model\n",
        "model_2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10,activation='relu'),\n",
        "    tf.keras.layers.Dense(1)                           \n",
        "]) \n",
        "#model compile\n",
        "model_2.compile(\n",
        "    loss = tf.keras.losses.mae,\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=0.1),  # change the Adam & learning rate\n",
        "    metrics = tf.keras.metrics.mae\n",
        ")\n",
        "#model fitting\n",
        "model_2.fit(tf.expand_dims(X,axis=-1),y,epochs=200) # increase the epochs that model can learning more times"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKCCGi5QEW-6",
        "outputId": "a3ff35a1-1dc1-4b88-9e9f-384b80c68eec"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 629ms/step - loss: 13.1351 - mean_absolute_error: 13.1351\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 10.3289 - mean_absolute_error: 10.3289\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.6250 - mean_absolute_error: 7.6250\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.0434 - mean_absolute_error: 5.0434\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 3.7138 - mean_absolute_error: 3.7138\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.4931 - mean_absolute_error: 4.4931\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.0261 - mean_absolute_error: 5.0261\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.0372 - mean_absolute_error: 5.0372\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.6260 - mean_absolute_error: 4.6260\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.8703 - mean_absolute_error: 3.8703\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.2817 - mean_absolute_error: 3.2817\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.3070 - mean_absolute_error: 4.3070\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.6017 - mean_absolute_error: 4.6017\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0653 - mean_absolute_error: 4.0653\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.0748 - mean_absolute_error: 3.0748\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.0016 - mean_absolute_error: 3.0016\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.3317 - mean_absolute_error: 3.3317\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.4704 - mean_absolute_error: 3.4704\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.3113 - mean_absolute_error: 3.3113\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.9168 - mean_absolute_error: 2.9168\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.3707 - mean_absolute_error: 2.3707\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.0521 - mean_absolute_error: 2.0521\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.0792 - mean_absolute_error: 2.0792\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.7429 - mean_absolute_error: 1.7429\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.3199 - mean_absolute_error: 1.3199\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4314 - mean_absolute_error: 1.4314\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.3654 - mean_absolute_error: 1.3654\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6739 - mean_absolute_error: 0.6739\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.0092 - mean_absolute_error: 1.0092\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.1945 - mean_absolute_error: 1.1945\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.0317 - mean_absolute_error: 1.0317\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.8096 - mean_absolute_error: 0.8096\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0329 - mean_absolute_error: 1.0329\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7609 - mean_absolute_error: 0.7609\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7372 - mean_absolute_error: 0.7372\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.9853 - mean_absolute_error: 0.9853\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8574 - mean_absolute_error: 0.8574\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4560 - mean_absolute_error: 0.4560\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7252 - mean_absolute_error: 0.7252\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9363 - mean_absolute_error: 0.9363\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6033 - mean_absolute_error: 0.6033\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4501 - mean_absolute_error: 0.4501\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8249 - mean_absolute_error: 0.8249\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8618 - mean_absolute_error: 0.8618\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5582 - mean_absolute_error: 0.5582\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5308 - mean_absolute_error: 0.5308\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5932 - mean_absolute_error: 0.5932\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5918 - mean_absolute_error: 0.5918\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4045 - mean_absolute_error: 0.4045\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3408 - mean_absolute_error: 0.3408\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3000 - mean_absolute_error: 0.3000\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4622 - mean_absolute_error: 0.4622\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4190 - mean_absolute_error: 0.4190\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2527 - mean_absolute_error: 0.2527\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2910 - mean_absolute_error: 0.2910\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2296 - mean_absolute_error: 0.2296\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2209 - mean_absolute_error: 0.2209\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2354 - mean_absolute_error: 0.2354\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.2152 - mean_absolute_error: 0.2152\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3506 - mean_absolute_error: 0.3506\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3360 - mean_absolute_error: 0.3360\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2697 - mean_absolute_error: 0.2697\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2614 - mean_absolute_error: 0.2614\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3038 - mean_absolute_error: 0.3038\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2246 - mean_absolute_error: 0.2246\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2045 - mean_absolute_error: 0.2045\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2374 - mean_absolute_error: 0.2374\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2499 - mean_absolute_error: 0.2499\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.1103 - mean_absolute_error: 0.1103\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2468 - mean_absolute_error: 0.2468\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.1011 - mean_absolute_error: 0.1011\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.1435 - mean_absolute_error: 0.1435\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2004 - mean_absolute_error: 0.2004\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.1571 - mean_absolute_error: 0.1571\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.1634 - mean_absolute_error: 0.1634\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1122 - mean_absolute_error: 0.1122\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2905 - mean_absolute_error: 0.2905\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.2359 - mean_absolute_error: 0.2359\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1745 - mean_absolute_error: 0.1745\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2611 - mean_absolute_error: 0.2611\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2126 - mean_absolute_error: 0.2126\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3052 - mean_absolute_error: 0.3052\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1238 - mean_absolute_error: 0.1238\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3448 - mean_absolute_error: 0.3448\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1890 - mean_absolute_error: 0.1890\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4310 - mean_absolute_error: 0.4310\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5127 - mean_absolute_error: 0.5127\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1150 - mean_absolute_error: 0.1150\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7199 - mean_absolute_error: 0.7199\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9916 - mean_absolute_error: 0.9916\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7600 - mean_absolute_error: 0.7600\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1154 - mean_absolute_error: 0.1154\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9135 - mean_absolute_error: 0.9135\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.3542 - mean_absolute_error: 1.3542\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.3116 - mean_absolute_error: 1.3116\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.8996 - mean_absolute_error: 0.8996\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2352 - mean_absolute_error: 0.2352\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.9261 - mean_absolute_error: 0.9261\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4183 - mean_absolute_error: 1.4183\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4642 - mean_absolute_error: 1.4642\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1003 - mean_absolute_error: 1.1003\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3968 - mean_absolute_error: 0.3968\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6612 - mean_absolute_error: 0.6612\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1980 - mean_absolute_error: 1.1980\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3184 - mean_absolute_error: 1.3184\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0594 - mean_absolute_error: 1.0594\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5158 - mean_absolute_error: 0.5158\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3961 - mean_absolute_error: 0.3961\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8913 - mean_absolute_error: 0.8913\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9983 - mean_absolute_error: 0.9983\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6923 - mean_absolute_error: 0.6923\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1923 - mean_absolute_error: 0.1923\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5624 - mean_absolute_error: 0.5624\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.8371 - mean_absolute_error: 0.8371\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7209 - mean_absolute_error: 0.7209\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2267 - mean_absolute_error: 0.2267\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5931 - mean_absolute_error: 0.5931\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.9229 - mean_absolute_error: 0.9229\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.8340 - mean_absolute_error: 0.8340\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4016 - mean_absolute_error: 0.4016\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3548 - mean_absolute_error: 0.3548\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7042 - mean_absolute_error: 0.7042\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6587 - mean_absolute_error: 0.6587\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3107 - mean_absolute_error: 0.3107\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4117 - mean_absolute_error: 0.4117\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6184 - mean_absolute_error: 0.6184\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4972 - mean_absolute_error: 0.4972\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2093 - mean_absolute_error: 0.2093\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3831 - mean_absolute_error: 0.3831\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4450 - mean_absolute_error: 0.4450\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1886 - mean_absolute_error: 0.1886\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4274 - mean_absolute_error: 0.4274\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5771 - mean_absolute_error: 0.5771\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3839 - mean_absolute_error: 0.3839\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2103 - mean_absolute_error: 0.2103\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4975 - mean_absolute_error: 0.4975\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4883 - mean_absolute_error: 0.4883\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1916 - mean_absolute_error: 0.1916\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3386 - mean_absolute_error: 0.3386\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4125 - mean_absolute_error: 0.4125\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1505 - mean_absolute_error: 0.1505\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2569 - mean_absolute_error: 0.2569\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2713 - mean_absolute_error: 0.2713\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1042 - mean_absolute_error: 0.1042\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1887 - mean_absolute_error: 0.1887\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1008 - mean_absolute_error: 0.1008\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1046 - mean_absolute_error: 0.1046\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2187 - mean_absolute_error: 0.2187\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1324 - mean_absolute_error: 0.1324\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3174 - mean_absolute_error: 0.3174\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3533 - mean_absolute_error: 0.3533\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0831 - mean_absolute_error: 0.0831\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4093 - mean_absolute_error: 0.4093\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5131 - mean_absolute_error: 0.5131\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2169 - mean_absolute_error: 0.2169\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4665 - mean_absolute_error: 0.4665\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6706 - mean_absolute_error: 0.6706\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4915 - mean_absolute_error: 0.4915\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1036 - mean_absolute_error: 0.1036\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4172 - mean_absolute_error: 0.4172\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4513 - mean_absolute_error: 0.4513\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1519 - mean_absolute_error: 0.1519\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5314 - mean_absolute_error: 0.5314\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7849 - mean_absolute_error: 0.7849\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7049 - mean_absolute_error: 0.7049\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2732 - mean_absolute_error: 0.2732\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5104 - mean_absolute_error: 0.5104\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.8577 - mean_absolute_error: 0.8577\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.8063 - mean_absolute_error: 0.8063\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3853 - mean_absolute_error: 0.3853\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3715 - mean_absolute_error: 0.3715\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.7028 - mean_absolute_error: 0.7028\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6702 - mean_absolute_error: 0.6702\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2884 - mean_absolute_error: 0.2884\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4432 - mean_absolute_error: 0.4432\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.7865 - mean_absolute_error: 0.7865\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7276 - mean_absolute_error: 0.7276\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3352 - mean_absolute_error: 0.3352\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5930 - mean_absolute_error: 0.5930\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6539 - mean_absolute_error: 0.6539\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5309 - mean_absolute_error: 0.5309\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2564 - mean_absolute_error: 0.2564\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6777 - mean_absolute_error: 0.6777\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7477 - mean_absolute_error: 0.7477\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4592 - mean_absolute_error: 0.4592\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2166 - mean_absolute_error: 0.2166\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5842 - mean_absolute_error: 0.5842\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5887 - mean_absolute_error: 0.5887\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2498 - mean_absolute_error: 0.2498\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3727 - mean_absolute_error: 0.3727\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4330 - mean_absolute_error: 0.4330\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2770 - mean_absolute_error: 0.2770\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5453 - mean_absolute_error: 0.5453\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6239 - mean_absolute_error: 0.6239\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4715 - mean_absolute_error: 0.4715\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1700 - mean_absolute_error: 0.1700\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4329 - mean_absolute_error: 0.4329\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3310 - mean_absolute_error: 0.3310\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2077 - mean_absolute_error: 0.2077\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2161 - mean_absolute_error: 0.2161\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa5250343d0>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.predict([17.0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1I1Jip5jPVc",
        "outputId": "fc73e75f-d38d-43c6-eb79-942556b7b210"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa521e54170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[27.69628]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "n7OM-giUjPXv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iqDjZJjEjPav"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "45wL9d5tjPhW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}